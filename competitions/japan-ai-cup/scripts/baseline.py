"""
Japan AI Cup - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
LightGBMã‚’ä½¿ç”¨ã—ãŸé¡§å®¢å†è¨ªäºˆæ¸¬
"""
import duckdb
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
import warnings
warnings.filterwarnings('ignore')

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = "competitions/japan-ai-cup/data"
OUTPUT_DIR = "competitions/japan-ai-cup/predictions"

print("=" * 60)
print("Japan AI Cup - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«")
print("=" * 60)

# =============================================================================
# 1. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆDuckDBã§é«˜é€Ÿå‡¦ç†ï¼‰
# =============================================================================
print("\nğŸ“Š ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°...")

con = duckdb.connect()

# å‚ç…§æ—¥ï¼ˆäºˆæ¸¬å¯¾è±¡æœŸé–“ã®é–‹å§‹æ—¥ï¼‰
REFERENCE_DATE = 20250203

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®ç‰¹å¾´é‡ã‚’é›†ç´„
features_query = f"""
WITH user_features AS (
    SELECT
        user_id,

        -- è³¼è²·è¡Œå‹•ã®çµ±è¨ˆ
        COUNT(*) as purchase_count,
        SUM(total_price) as total_spent,
        AVG(total_price) as avg_spent,
        MAX(total_price) as max_spent,
        MIN(total_price) as min_spent,
        STDDEV(total_price) as std_spent,

        -- å•†å“æ•°ãƒ»ç‚¹æ•°
        SUM(amount) as total_items,
        AVG(amount) as avg_items,

        -- æ¥åº—ãƒ‘ã‚¿ãƒ¼ãƒ³
        COUNT(DISTINCT date) as visit_days,
        MIN(date) as first_purchase_date,
        MAX(date) as last_purchase_date,
        {REFERENCE_DATE} - MAX(date) as recency,

        -- å•†å“ã‚«ãƒ†ã‚´ãƒªã®å¤šæ§˜æ€§
        COUNT(DISTINCT item_category_cd_1) as unique_cat1,
        COUNT(DISTINCT item_category_cd_2) as unique_cat2,
        COUNT(DISTINCT item_category_cd_3) as unique_cat3,
        COUNT(DISTINCT jan_cd) as unique_products,

        -- é¡§å®¢å±æ€§ï¼ˆæœ€æ–°ã®å€¤ã‚’å–å¾—ï¼‰
        FIRST(age_category) as age_category,
        FIRST(sex) as sex,
        FIRST(user_stage) as user_stage,
        FIRST(membership_start_ym) as membership_start_ym,
        FIRST(user_flag_ec) as user_flag_ec,
        FIRST(user_flag_1) as user_flag_1,
        FIRST(user_flag_2) as user_flag_2,
        FIRST(user_flag_3) as user_flag_3,
        FIRST(user_flag_4) as user_flag_4,
        FIRST(user_flag_5) as user_flag_5,
        FIRST(user_flag_6) as user_flag_6

    FROM read_csv_auto('{DATA_DIR}/data.csv')
    GROUP BY user_id
)
SELECT
    f.*,
    -- æ´¾ç”Ÿç‰¹å¾´é‡
    f.total_spent / NULLIF(f.visit_days, 0) as avg_spent_per_visit,
    f.purchase_count / NULLIF(f.visit_days, 0) as avg_purchases_per_visit,
    f.last_purchase_date - f.first_purchase_date as purchase_span,
    CASE
        WHEN f.last_purchase_date - f.first_purchase_date > 0
        THEN f.visit_days * 1.0 / (f.last_purchase_date - f.first_purchase_date)
        ELSE 0
    END as visit_frequency
FROM user_features f
"""

df_features = con.execute(features_query).fetchdf()
print(f"  ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿: {len(df_features):,} ãƒ¦ãƒ¼ã‚¶ãƒ¼, {len(df_features.columns)} ã‚«ãƒ©ãƒ ")

# Train/Test ãƒ©ãƒ™ãƒ«ã®èª­ã¿è¾¼ã¿
df_train_labels = con.execute(f"""
    SELECT user_id, churn as target
    FROM read_csv_auto('{DATA_DIR}/train_flag.csv')
""").fetchdf()

df_test_users = con.execute(f"""
    SELECT user_id
    FROM read_csv_auto('{DATA_DIR}/sample_submission.csv')
""").fetchdf()

con.close()

# =============================================================================
# 2. Train/Test ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
# =============================================================================
print("\nğŸ”„ Train/Test ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™...")

# Trainã¨Testã«åˆ†å‰²
df_train = df_features.merge(df_train_labels, on='user_id', how='inner')
df_test = df_features.merge(df_test_users, on='user_id', how='inner')

print(f"  Train: {len(df_train):,} ãƒ¦ãƒ¼ã‚¶ãƒ¼")
print(f"  Test: {len(df_test):,} ãƒ¦ãƒ¼ã‚¶ãƒ¼")

# ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã®å®šç¾©
feature_cols = [col for col in df_features.columns if col != 'user_id']

# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡
categorical_cols = ['age_category', 'sex', 'user_stage']

# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã‚’categoryå‹ã«å¤‰æ›
for col in categorical_cols:
    df_train[col] = df_train[col].astype('category')
    df_test[col] = df_test[col].astype('category')

# æ¬ æå€¤ã®ç¢ºèª
print(f"\n  æ¬ æå€¤ã®ã‚ã‚‹ã‚«ãƒ©ãƒ :")
for col in feature_cols:
    null_count = df_train[col].isnull().sum()
    if null_count > 0:
        print(f"    {col}: {null_count:,} ({null_count/len(df_train):.1%})")

# =============================================================================
# 3. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆLightGBM + ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
# =============================================================================
print("\nğŸš€ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’...")

X_train = df_train[feature_cols]
y_train = df_train['target']
X_test = df_test[feature_cols]

# LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'learning_rate': 0.05,
    'num_leaves': 31,
    'max_depth': -1,
    'min_child_samples': 20,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1,
    'seed': 42,
}

# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
n_splits = 5
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

oof_preds = np.zeros(len(X_train))
test_preds = np.zeros(len(X_test))
cv_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), 1):
    print(f"\n  Fold {fold}/{n_splits}")

    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

    train_data = lgb.Dataset(X_tr, label=y_tr, categorical_feature=categorical_cols)
    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_cols)

    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[train_data, val_data],
        valid_names=['train', 'valid'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=0)
        ]
    )

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬
    val_pred = model.predict(X_val)
    oof_preds[val_idx] = val_pred

    # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ï¼ˆå¹³å‡ã‚’å–ã‚‹ï¼‰
    test_preds += model.predict(X_test) / n_splits

    # ã‚¹ã‚³ã‚¢è¨ˆç®—
    fold_auc = roc_auc_score(y_val, val_pred)
    cv_scores.append(fold_auc)
    print(f"    AUC: {fold_auc:.5f}")

# å…¨ä½“ã®CV ã‚¹ã‚³ã‚¢
overall_auc = roc_auc_score(y_train, oof_preds)
print(f"\n{'=' * 60}")
print(f"ğŸ“ˆ CVçµæœ")
print(f"{'=' * 60}")
print(f"  å„Foldã® AUC: {[f'{s:.5f}' for s in cv_scores]}")
print(f"  å¹³å‡ AUC: {np.mean(cv_scores):.5f} (Â±{np.std(cv_scores):.5f})")
print(f"  OOF AUC: {overall_auc:.5f}")

# =============================================================================
# 4. ç‰¹å¾´é‡é‡è¦åº¦
# =============================================================================
print(f"\n{'=' * 60}")
print("ğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ (Top 15)")
print(f"{'=' * 60}")

importance = model.feature_importance(importance_type='gain')
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': importance
}).sort_values('importance', ascending=False)

for i, row in feature_importance.head(15).iterrows():
    pct = row['importance'] / importance.sum() * 100
    print(f"  {row['feature']}: {pct:.2f}%")

# =============================================================================
# 5. æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
# =============================================================================
print(f"\n{'=' * 60}")
print("ğŸ“ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ")
print(f"{'=' * 60}")

submission = pd.DataFrame({
    'user_id': df_test['user_id'],
    'pred': test_preds
})

output_path = f"{OUTPUT_DIR}/submission.csv"
submission.to_csv(output_path, index=False)

print(f"  ä¿å­˜å…ˆ: {output_path}")
print(f"  è¡Œæ•°: {len(submission):,}")
print(f"  äºˆæ¸¬å€¤ã®ç¯„å›²: {submission['pred'].min():.4f} ã€œ {submission['pred'].max():.4f}")
print(f"  äºˆæ¸¬å€¤ã®å¹³å‡: {submission['pred'].mean():.4f}")

print("\nâœ… ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«å®Œäº†")
